{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be74f4be-2c1e-4500-8fd5-063d340b72f1",
   "metadata": {},
   "source": [
    "# Bedrock DeepSeek-R1 Model Support\n",
    "In this notebook we quickly explore how you can leverage the popular [DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1#usage-recommendations) model via Amazon Bedrock. Within Bedrock this model is now availble as a fully managed serverless option that you can invoke via the standard [Runtime Client] InvokeModel API call(https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime.html) or even more simply through the [Converse API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html).\n",
    "\n",
    "## Credits/References & Additional Resources\n",
    "- [Offical AWS Blog](https://aws.amazon.com/blogs/aws/deepseek-r1-now-available-as-a-fully-managed-serverless-model-in-amazon-bedrock/)\n",
    "- [DeepSeek with Bedrock Guardrails](https://aws.amazon.com/blogs/machine-learning/protect-your-deepseek-model-deployments-with-amazon-bedrock-guardrails/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdb9bcc-b574-43c1-b776-a47d92a52f3e",
   "metadata": {},
   "source": [
    "## Sample Invocation Via Invoke Model API Call\n",
    "Docs: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/invoke_model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6bd2df-9426-4c1f-a42b-80c8fb545af4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "# Setup Bedrock runtime client, working in us-west-2\n",
    "runtime = boto3.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "model_id = \"us.deepseek.r1-v1:0\"\n",
    "\n",
    "# Sample payload, replace with your request\n",
    "text_payload = {\n",
    "    \"prompt\": \"<｜begin_of_sentence｜><｜User｜>Who is Roger Federer?<｜Assistant｜><think>\\n\",\n",
    "    \"max_tokens\": 1000,\n",
    "    \"temperature\": 0.6,\n",
    "    \"top_p\": 0.9\n",
    "}\n",
    "body_payload = json.dumps(text_payload)\n",
    "\n",
    "# sample inference\n",
    "response = runtime.invoke_model(\n",
    "    body=body_payload,\n",
    "    modelId=model_id,\n",
    "    accept=\"application/json\",\n",
    "    contentType=\"application/json\"\n",
    ")\n",
    "response_body = json.loads(response[\"body\"].read())\n",
    "generated_text = response_body.get(\"choices\", [{}])[0].get(\"text\", \"\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36af53ef-c6ff-4084-aa4d-0aecc1adaf9d",
   "metadata": {},
   "source": [
    "## Invoke Via Converse API\n",
    "Provides a standard format across different models where you don't have to adjust the payload structuring depending on the model provider, makes it simple to test the same payload across different model providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f2fbf8-973c-4af4-ac55-dc5a57d78bc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Structure payload, replace user_message with your request\n",
    "user_message = \"Who is Roger Federer\"\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": user_message}],\n",
    "    }\n",
    "]\n",
    "\n",
    "# sample inference via converse API, uses sample client different method\n",
    "response = runtime.converse(\n",
    "    modelId=model_id,\n",
    "    messages=conversation,\n",
    "    inferenceConfig={\"maxTokens\": 2000, \"temperature\": 0.6},\n",
    ")\n",
    "response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e49b82-9db5-48e6-bdc5-4b54475f5d90",
   "metadata": {},
   "source": [
    "## Sample Inference with HuggingFace (HF) Math Dataset\n",
    "Here we iterate over a sample HF math dataset to showcase R1's advanced reasoning capabilities, you can play with different datasets here to test the capabilities. In coming sections we will also explore how we can evaluate LLMs across different tasks.\n",
    "\n",
    "- <b>Dataset</b>: https://huggingface.co/datasets/microsoft/orca-math-word-problems-200k\n",
    "- <b>License</b>: MIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae9ed30-0d0f-44f1-b3f6-84af89e82e57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install datasets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f712e27b-c117-4537-8a00-ff68d5ef5dc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load sample dataset to work with\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"microsoft/orca-math-word-problems-200k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e542090-93c8-4441-9ad9-433b2c2f4675",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample payload/input question\n",
    "ds['train'][1:10]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392ad2ed-a31e-4670-bbc6-34f761d05706",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wrap invoke model API call into a function to simplify code a little as we iterate over the dataset\n",
    "def inference(question: str) -> str:\n",
    "    text_payload = {\n",
    "        \"prompt\": f\"<｜begin_of_sentence｜><｜User｜>{question}<｜Assistant｜><think>\\n\",\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0.6,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    "    body_payload = json.dumps(text_payload)\n",
    "    # invoke_model API call\n",
    "    response = runtime.invoke_model(\n",
    "        body=body_payload,\n",
    "        modelId=model_id,\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "    #parse model output\n",
    "    response_body = json.loads(response[\"body\"].read())\n",
    "    generated_text = response_body.get(\"choices\", [{}])[0].get(\"text\", \"\")\n",
    "    return generated_text\n",
    "\n",
    "# iterate over some samples in the dataset and run inference with the DeepSeek R-1 model\n",
    "for i,question in enumerate(ds['train'][1:10]['question'], start = 1):\n",
    "    print(f\"Question {i}: {question}\")\n",
    "    model_output = inference(question)\n",
    "    print(f\"Generated Answer: {model_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
