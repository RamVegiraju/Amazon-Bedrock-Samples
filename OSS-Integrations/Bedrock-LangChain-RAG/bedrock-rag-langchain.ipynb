{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95b72388-dac8-43f2-9966-34378bd52d46",
   "metadata": {},
   "source": [
    "# Bedrock Implementation of Medium Analyzer\n",
    "\n",
    "In this notebook we will use Claude for the LLM via Bedrock and the Titan Embeddings Model to build out a simple RAG Workflow orchestrated by LangChain.\n",
    "\n",
    "## Credits/Additional References\n",
    "- <b>Bedrock OSS RAG Original Implementation</b>: https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/06_OpenSource_examples/01_Langchain_KnowledgeBases_and_RAG_examples/01_qa_w_rag_claude.ipynb\n",
    "- <b>Bedrock Workshop Official Open Source Samples</b>: https://github.com/aws-samples/amazon-bedrock-workshop/tree/main/06_OpenSource_examples\n",
    "- <b>Bedrock AWS Samples Repo</b>: https://github.com/aws-samples/amazon-bedrock-samples/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18ead9a-da96-4d75-b992-d0104496edd9",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Can use any Python environment that has Boto3 access to the Bedrock models, in this case we use a SageMaker classic notebook instance with an ml.c5.xlarge CPU instance. If working in a GPU environment ensure to install faiss-gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9571901e-6d15-4118-b311-4d15f91b1837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install langchain-community faiss-cpu==1.8.0 langchain pypdf #restart kernel after installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd577f86-3dbf-4826-bf7a-9d4df28ec3fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import langchain\n",
    "from langchain.embeddings.cache import CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "boto3_bedrock = boto3.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef354f7f-9a15-434f-8afd-293076530ce1",
   "metadata": {},
   "source": [
    "## Sample Boto3 Inference With Claude V2\n",
    "Can update this Claude model to the 3.x family if needed, here's how a sample API call looks like via boto3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2da9a9-2660-44bd-82a4-addc3447bbf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "model_id = 'anthropic.claude-v2'\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "prompt_data = \"\"\"Human: Write me a small paragraph saying nice things about me.\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "print(prompt_data)\n",
    "\n",
    "body = json.dumps({\"prompt\": prompt_data, \"max_tokens_to_sample\": 500})\n",
    "response = boto3_bedrock.invoke_model(\n",
    "    body=body, modelId=model_id, accept=accept, contentType=contentType\n",
    ")\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "print(response_body.get(\"completion\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7682a7d-4cf4-4408-ba03-d1f47ea110e5",
   "metadata": {},
   "source": [
    "## Embeddings & Vector Store Setup\n",
    "In this case for the RAG stack we use the following:\n",
    "\n",
    "- <b>Embeddings Model</b>: Amazon Titan via Bedrock\n",
    "- <b>Vector Store</b>: FAISS\n",
    "\n",
    "Ensure that you have the sagemaker-articles directly cloned as well or replace this with your own set of PDF documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03ace06-4fbf-486e-9a73-19991d201634",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "# where our embeddings will be stored\n",
    "store = LocalFileStore(\"./cache/\")\n",
    "\n",
    "# instantiate a loader: this loads our data, use PDF in this case\n",
    "loader = PyPDFDirectoryLoader(\"sagemaker-articles/\")\n",
    "\n",
    "# by default the PDF loader both loads and splits the documents for us\n",
    "pages = loader.load_and_split()\n",
    "print(len(pages))\n",
    "\n",
    "# create the LLM and Embeddings Models object via LangChain\n",
    "llm = Bedrock(model_id=\"anthropic.claude-v2\", client=boto3_bedrock, model_kwargs={'max_tokens_to_sample':200})\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=boto3_bedrock)\n",
    "\n",
    "# pass in our vector store\n",
    "embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    bedrock_embeddings,\n",
    "    store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dbec83-8dc3-49cc-ade4-ea3149c0fa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate vector store, we use FAISS in this case\n",
    "vector_store = FAISS.from_documents(pages, embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd7bf95-7116-4298-bff5-9644e6e9b63d",
   "metadata": {},
   "source": [
    "## Chain Creation & Inference\n",
    "\n",
    "We can wrap all these objects into a singular Retrieval QA chain (RAG): https://python.langchain.com/api_reference/langchain/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c27d224-bdab-439f-8dca-7e388b855f3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Promot Template for the Retrieval QA chain\n",
    "def fill_prompt(template, human_text):\n",
    "    # Replace the placeholder 'Human:' with the provided human_text\n",
    "    filled_prompt = template.replace(\"Human:\", f\"Human: {human_text}\")\n",
    "    return filled_prompt\n",
    "\n",
    "# Claude structured template\n",
    "prompt_data = \"\"\"Human:\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "\n",
    "# this is the entire retrieval system\n",
    "from langchain.chains import RetrievalQA\n",
    "medium_qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700b0765-e522-4105-bca1-49b0a499b9d5",
   "metadata": {},
   "source": [
    "## Sample Inference with RAG and Vanilla Bedrock Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6451d0e7-8cf7-46ba-86a2-9e69e52fd54d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_prompts = [\"What does Ram Vegiraju write about?\",\n",
    "                 \"What is Amazon SageMaker?\",\n",
    "                 \"What is Amazon SageMaker Inference?\",\n",
    "                 \"What are the different hosting options for Amazon SageMaker?\",\n",
    "                 \"What is Serverless Inference with Amazon SageMaker?\",\n",
    "                 \"What's the difference between Multi-Model Endpoints and Multi-Container Endpoints?\",\n",
    "                 \"What SDKs can I use to work with Amazon SageMaker?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9e5621-9215-4bfa-8c7d-1cf050e87e4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for prompt in sample_prompts:\n",
    "    print(prompt)\n",
    "    print(\"------------------------------------\")\n",
    "    print(\"Vanilla Bedrock Response\")\n",
    "    print(\"------------------------------------\")\n",
    "    prompt_template = fill_prompt(prompt_data, prompt)\n",
    "    body = json.dumps({\"prompt\": prompt_template, \"max_tokens_to_sample\": 500})\n",
    "    response = boto3_bedrock.invoke_model(\n",
    "        body=body, modelId=model_id, accept=accept, contentType=contentType\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    print(response_body.get(\"completion\"))\n",
    "    print(\"------------------------------------\")\n",
    "    print(\"RAG Enabled Response\")\n",
    "    print(\"------------------------------------\")\n",
    "    response_rag = medium_qa_chain({\"query\":prompt})\n",
    "    print(response_rag['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
